{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4f12b35-c313-47e5-9d9e-e1fcc3650073",
   "metadata": {},
   "source": [
    "## Setting Up the API Connection and Fetching Evaluation Questions\n",
    "\n",
    "In this section, we test the evaluation API and implement a function to retrieve all available questions. I use a simple `GET` request to obtain the test dataset and then display a random sample to understand the structure of each question object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39771cbe-6ff9-42ca-9992-3185536492bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"task_id\": \"cabe07ed-9eca-40ea-8ead-410ef5e83f91\",\n",
      "        \"question\": \"What is the surname of the equine veterinarian mentioned in 1.E Exercises from the chemistry materials licensed by Marisa Alviar-Agnew & Henry Agnew under the CK-12 license in LibreText's Introductory Chemistry materials as compiled 08/21/2023?\",\n",
      "        \"Level\": \"1\",\n",
      "        \"file_name\": \"\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import random\n",
    "import json\n",
    "\n",
    "BASE_URL = \"https://agents-course-unit4-scoring.hf.space\"\n",
    "\n",
    "def fetch_questions():\n",
    "    response = requests.get(f\"{BASE_URL}/questions\", headers={\"accept\": \"application/json\"})\n",
    "    if response.ok:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(\"Failed to fetch questions:\", response.status_code)\n",
    "        return []\n",
    "\n",
    "# Get the questions\n",
    "questions = fetch_questions()\n",
    "\n",
    "# Print the first one\n",
    "if questions:\n",
    "    # random.seed(42)\n",
    "    random_samples = random.sample(questions, 1)\n",
    "\n",
    "    json_str = json.dumps(random_samples, indent=4)    \n",
    "    print(json_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1091d004-b256-4bbc-a3c9-0f9553175fac",
   "metadata": {},
   "source": [
    "## Identifying Questions with Associated Resource Files\n",
    "\n",
    "Some function might include additional resource files, such as documents or datasets. These files are necessary for answering or evaluating the task correctly. Here, I filter the list of questions in the dataset that requires downloading external resource.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3296c1c8-b19c-4d55-a1fc-9fe08abb1560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task ID: cca530fc-4052-43b2-b130-b30968d8aa44\n",
      "File: cca530fc-4052-43b2-b130-b30968d8aa44.png\n",
      "\n",
      "Task ID: 99c9cc74-fdc8-46c6-8f8d-3ce2d3bfeea3\n",
      "File: 99c9cc74-fdc8-46c6-8f8d-3ce2d3bfeea3.mp3\n",
      "\n",
      "Task ID: f918266a-b3e0-4914-865d-4faa564f1aef\n",
      "File: f918266a-b3e0-4914-865d-4faa564f1aef.py\n",
      "\n",
      "Task ID: 1f975693-876d-457b-a649-393859e79bf3\n",
      "File: 1f975693-876d-457b-a649-393859e79bf3.mp3\n",
      "\n",
      "Task ID: 7bd855d8-463d-4ed5-93ca-5fe35145f733\n",
      "File: 7bd855d8-463d-4ed5-93ca-5fe35145f733.xlsx\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for q in questions:\n",
    "    v = list(q.values())\n",
    "    if bool(v[3]):\n",
    "        print(f\"Task ID: {v[0]}\")\n",
    "        print(f\"File: {v[3]}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488f8a7c-fe34-4aa5-b4c0-45e921725996",
   "metadata": {},
   "source": [
    "## Downloading Associated Files for Relevant Tasks\n",
    "\n",
    "Now that we know which questions include external files, I implement a function to download those resources. This ensures that we have all necessary data locally before we attempt to generate or score responses. I create a `resources/` directory and save each file using its original filename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5faa8933-56eb-4105-9757-c535ae76e121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task ID: cca530fc-4052-43b2-b130-b30968d8aa44\n",
      "File: cca530fc-4052-43b2-b130-b30968d8aa44.png\n",
      "Downloaded file for task cca530fc-4052-43b2-b130-b30968d8aa44 -> resources/cca530fc-4052-43b2-b130-b30968d8aa44.png\n",
      "\n",
      "Task ID: 99c9cc74-fdc8-46c6-8f8d-3ce2d3bfeea3\n",
      "File: 99c9cc74-fdc8-46c6-8f8d-3ce2d3bfeea3.mp3\n",
      "Downloaded file for task 99c9cc74-fdc8-46c6-8f8d-3ce2d3bfeea3 -> resources/99c9cc74-fdc8-46c6-8f8d-3ce2d3bfeea3.mp3\n",
      "\n",
      "Task ID: f918266a-b3e0-4914-865d-4faa564f1aef\n",
      "File: f918266a-b3e0-4914-865d-4faa564f1aef.py\n",
      "Downloaded file for task f918266a-b3e0-4914-865d-4faa564f1aef -> resources/f918266a-b3e0-4914-865d-4faa564f1aef.py\n",
      "\n",
      "Task ID: 1f975693-876d-457b-a649-393859e79bf3\n",
      "File: 1f975693-876d-457b-a649-393859e79bf3.mp3\n",
      "Downloaded file for task 1f975693-876d-457b-a649-393859e79bf3 -> resources/1f975693-876d-457b-a649-393859e79bf3.mp3\n",
      "\n",
      "Task ID: 7bd855d8-463d-4ed5-93ca-5fe35145f733\n",
      "File: 7bd855d8-463d-4ed5-93ca-5fe35145f733.xlsx\n",
      "Downloaded file for task 7bd855d8-463d-4ed5-93ca-5fe35145f733 -> resources/7bd855d8-463d-4ed5-93ca-5fe35145f733.xlsx\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def download_file(task_id, filename=None):\n",
    "    response = requests.get(f\"{BASE_URL}/files/{task_id}\")\n",
    "    if response.ok:\n",
    "        filename = filename or f\"{task_id}.zip\"\n",
    "        filename = f\"resources/{filename}\"\n",
    "        with open(filename, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"Downloaded file for task {task_id} -> {filename}\")\n",
    "    else:\n",
    "        print(f\"No file found or failed to download for task {task_id}\")\n",
    "\n",
    "for q in questions:\n",
    "    v = list(q.values())\n",
    "    if bool(v[3]):\n",
    "        task_id = v[0]\n",
    "        file = v[3]\n",
    "        print(f\"Task ID: {task_id}\")\n",
    "        print(f\"File: {file}\")\n",
    "\n",
    "        download_file(task_id, file)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c4eff4-6d5c-489e-80ea-0f9e1a2bcc59",
   "metadata": {},
   "source": [
    "## Exploring the API’s Random Question Endpoint\n",
    "\n",
    "In addition to fetching the full set of questions, the API also offers an endpoint for retrieving a random question. This is useful for sampling tasks dynamically during development or testing, without processing the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42797e80-ac51-434a-b47b-18aede83a25a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"task_id\": \"cabe07ed-9eca-40ea-8ead-410ef5e83f91\",\n",
      "        \"question\": \"What is the surname of the equine veterinarian mentioned in 1.E Exercises from the chemistry materials licensed by Marisa Alviar-Agnew & Henry Agnew under the CK-12 license in LibreText's Introductory Chemistry materials as compiled 08/21/2023?\",\n",
      "        \"Level\": \"1\",\n",
      "        \"file_name\": \"\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "def fetch_random_question():\n",
    "    response = requests.get(f\"{BASE_URL}/random-question\", headers={\"accept\": \"application/json\"})\n",
    "    if response.ok:\n",
    "        return response.json()\n",
    "    else:\n",
    "        print(\"Failed to fetch questions:\", response.status_code)\n",
    "        return []\n",
    "\n",
    "res = fetch_random_question()\n",
    "json_str = json.dumps(random_samples, indent=4)    \n",
    "print(json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3adb042-0c6b-4469-9925-455a44871ef0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(questions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f794d11f-b933-4a5b-b238-7aa390556821",
   "metadata": {},
   "source": [
    "## Downloading the Training Set\n",
    "\n",
    "To effectively develop and test our agent, we need access to a set of pre-annotated tasks with corresponding gold answers. This \"training set\" provides labeled data that can be used to analyze model behavior, measure performance, or fine-tune the agent. Let's download the full training dataset from the API and examine its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18202010-8036-4513-8b11-f85dad6ef797",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"gaia-benchmark/GAIA\", name=\"2023_all\", split=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f94afe99-df33-4c3d-82f6-c1b174dd5b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset info:\n",
      " ├── Validation split: 165 samples\n",
      "────────────────────────────────────────────────────────────\n",
      "Task ID     : 65afbc8a-89ca-4ad5-8d62-355bb401f61d\n",
      "Level       : 1\n",
      "Question    : You are given this Excel file as a map. You start on the START cell and move toward the END cell. You are allowed to move two cells per turn, and you may move up, down, left, or right. You may not move fewer than two cells, and you may not move backward. You must avoid moving onto any blue cells. On the eleventh turn, what is the 6-digit hex code (without prefix) of the color of the cell where you land after moving?\n",
      "Answer      : F478A7\n",
      "Annotator Metadata:\n",
      " ├── Steps:\n",
      " │    ├── 1. Opened Map.xlsx.\n",
      " │    ├── 2. Counted 11 turns of 2 spaces each (22 spaces) along the path of non-blue cells.\n",
      " │    ├── 3. Opened cell formatting for the cell.\n",
      " │    ├── 4. Clicked the \"Fill\" tab.\n",
      " │    ├── 5. Clicked \"More Colors...\"\n",
      " │    ├── 6. Noted the hex code of the color.\n",
      " ├── Number of steps : 6\n",
      " ├── Duration        : 5 minutes\n",
      " ├── Tools used:\n",
      " │    ├── 1. Access to Excel files\n",
      " │    ├── 2. Color recognition\n",
      " │    ├── 3. Calculator (or ability to count)\n",
      " └── Number of tools : 3\n",
      "────────────────────────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset info:\")\n",
    "print(f\" ├── Validation split: {len(dataset)} samples\")\n",
    "\n",
    "sample = random.choice(dataset.to_list())\n",
    "\n",
    "# Pretty print sample\n",
    "line = \"─\" * 60\n",
    "print(line)\n",
    "print(f\"Task ID     : {sample['task_id']}\")\n",
    "print(f\"Level       : {sample['Level']}\")\n",
    "print(f\"Question    : {sample['Question']}\")\n",
    "print(f\"Answer      : {sample['Final answer']}\")\n",
    "print(\"Annotator Metadata:\")\n",
    "print(\" ├── Steps:\")\n",
    "for step in sample['Annotator Metadata']['Steps'].split('\\n'):\n",
    "    print(f\" │    ├── {step}\")\n",
    "print(f\" ├── Number of steps : {sample['Annotator Metadata']['Number of steps']}\")\n",
    "print(f\" ├── Duration        : {sample['Annotator Metadata']['How long did this take?']}\")\n",
    "print(f\" ├── Tools used:\")\n",
    "for tool in sample['Annotator Metadata']['Tools'].split('\\n'):\n",
    "    print(f\" │    ├── {tool}\")\n",
    "print(f\" └── Number of tools : {sample['Annotator Metadata']['Number of tools']}\")\n",
    "print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c26e17-8f86-4720-be34-94e28b0554e3",
   "metadata": {},
   "source": [
    "## Analyzing Tool Usage Across Annotated Tasks\n",
    "\n",
    "Before designing an agent capable of solving GAIA tasks, it's helpful to understand the types of tools that are required to fulfill the tasks. This insight informs us about the functional capabilities the agent may need, such as web browsing, calculation, or image recognition.\n",
    "\n",
    "Below, I extract and summarize tool usage statistics from the \"Annotator Metadata\" field of each annotated sample in the dataset. By standardizing the tool names and counting their frequencies, we can identify which tools were most commonly required and, by extension, which functionalities are essential for agent reasoning and task completion.\n",
    "\n",
    "This analysis helps prioritize which tool integrations should be implemented or optimized in the agent. For example, if most tasks rely on web search or calculators, these capabilities should be central to the agent's design and evaluation loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d81b43b7-6d5f-45d1-8416-d9135d9b046c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('web browser', 107), ('search engine', 101), ('calculator', 34), ('image recognition tools', 12), ('ne', 9), ('pdf access', 7), ('pdf viewer', 7), ('a web browser', 7), ('a search engine', 7), ('microsoft excel', 5), ('image recognition', 5), ('a calculator', 5), ('ocr', 4), ('python', 3), ('video recognition tools', 3), ('microsoft excel / google sheets', 3), ('excel', 3), ('color recognition', 3), ('excel file access', 3), ('access to wikipedia', 3), ('image recognition/ocr', 3), ('a file interface', 3), ('a web browser.', 2), ('a search engine.', 2), ('file handling', 2), ('a speech-to-text tool', 2), ('audio capability', 2), ('image recognition tools (to identify and parse a figure with three axes)', 1), ('unlambda compiler (optional)', 1), ('a calculator.', 1), ('google search', 1), ('jsonld file access', 1), ('video parsing', 1), ('python compiler', 1), ('word document access', 1), ('tool to extract text from images', 1), ('a word reversal tool / script', 1), ('counter', 1), ('xml file access', 1), ('access to the internet archive, web.archive.org', 1), ('text processing/diff tool', 1), ('gif parsing tools', 1), ('code/data analysis tools', 1), ('pdf reader', 1), ('markdown', 1), ('google translate access', 1), ('bass note data', 1), ('text editor', 1), ('xlsx file access', 1), ('powerpoint viewer', 1), ('csv file access', 1), ('calculator (or use excel)', 1), ('computer algebra system', 1), ('video processing software', 1), ('audio processing software', 1), ('computer vision', 1), ('google maps', 1), ('access to excel files', 1), ('calculator (or ability to count)', 1), ('a python ide', 1), ('spreadsheet editor', 1), ('tools required', 1), ('b browser', 1), ('image recognition and processing tools', 1), ('computer vision or ocr', 1), ('c++ compiler', 1), ('access to google maps', 1), ('youtube player', 1), ('natural language processor', 1), ('graph interaction tools', 1), ('bablyonian cuniform -> arabic legend', 1), ('access to youtube', 1), ('image search tools', 1), ('calculator or counting function', 1), ('a speech-to-text audio processing tool', 1), ('access to academic journal websites', 1), ('pdf reader/extracter', 1), (\"rubik's cube model\", 1), ('wikipedia', 1), ('video capability', 1), ('image processing tools', 1), ('age recognition software', 1), ('youtube', 1)])\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter, OrderedDict\n",
    "\n",
    "def extract_gaia_tool_stats(dataset):\n",
    "    tools = []\n",
    "    for sample in dataset:\n",
    "        raw_tools = sample.get(\"Annotator Metadata\", {}).get(\"Tools\", \"\")\n",
    "        for tool in raw_tools.split('\\n'):\n",
    "            tool = tool[2:].strip().lower()  # remove bullet/indentation\n",
    "            if tool.startswith(\"(\"):        # handles: \"(1) Wikipedia\"\n",
    "                tool = tool[tool.find(\")\") + 1:].strip()\n",
    "            if tool:  # skip empty entries\n",
    "                tools.append(tool)\n",
    "\n",
    "    # Count and sort tools by frequency\n",
    "    tools_counter = OrderedDict(sorted(Counter(tools).items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "    return tools_counter\n",
    "\n",
    "tools_counter = extract_gaia_tool_stats(dataset)\n",
    "print(tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a64c301-d545-4e04-a0f9-2a383c38109e",
   "metadata": {},
   "source": [
    "## Grouping Redundant Tools with Semantic Clustering\n",
    "\n",
    "The raw tool frequency data extracted previously includes many redundant or inconsistent tool names, such as `\"web browser\"`, `\"a web browser\"`, and `\"b browser\"`, which all refer to essentially the same capability. To clean this data and better understand the agent capabilities required, we group semantically similar tools using sentence embeddings and clustering.\n",
    "\n",
    "In this step, we use a pretrained sentence transformer (all-MiniLM-L6-v2) to embed each tool name into a vector space. Then, we apply agglomerative clustering based on cosine distance to group tool names that are semantically close to each other. For each cluster, we aggregate the total count of tool occurrences and print a sorted summary.\n",
    "\n",
    "This clustering helps us identify broader tool categories (e.g., search, calculation, image parsing) and reduces noise from naming inconsistencies in the annotations. These grouped categories will inform how we define and prioritize tool implementations for our LangGraph agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "29f680e7-23b2-4567-9f9a-20b9e93ebaf2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "def group_similar_tools(tools_counter, threshold=1.5):\n",
    "    \"\"\"\n",
    "    Group similar tool names based on semantic similarity using Agglomerative Clustering.\n",
    "    Returns dict {canonical_label: [tool_names...]} and {canonical_label: total_count}.\n",
    "    \"\"\"\n",
    "    tool_names = list(tools_counter.keys())\n",
    "    counts = list(tools_counter.values())\n",
    "\n",
    "    model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    embeddings = model.encode(tool_names)\n",
    "\n",
    "    # Perform clustering with cosine distance\n",
    "    clustering = AgglomerativeClustering(\n",
    "        n_clusters=None,\n",
    "        distance_threshold=threshold,\n",
    "        metric='cosine',        # <- Fix here\n",
    "        linkage='average'\n",
    "    )\n",
    "    labels = clustering.fit_predict(embeddings)\n",
    "\n",
    "    # Group by cluster\n",
    "    clusters = defaultdict(list)\n",
    "    cluster_counts = defaultdict(int)\n",
    "\n",
    "    for i, label in enumerate(labels):\n",
    "        clusters[label].append(tool_names[i])\n",
    "        cluster_counts[label] += counts[i]\n",
    "\n",
    "    # Build output\n",
    "    grouped = {\n",
    "        \", \".join(cluster): count\n",
    "        for cluster, count in zip(clusters.values(), cluster_counts.values())\n",
    "    }\n",
    "\n",
    "    # Sort by count\n",
    "    sorted_grouped = dict(sorted(grouped.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "    return sorted_grouped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7b7cee42-0e6f-4666-9353-24377d7e87d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ├── [web browser, search engine, a web browser, a search engine, access to wikipedia, a web browser., a search engine., google search, access to the internet archive, web.archive.org, b browser, youtube player, access to youtube, access to academic journal websites, wikipedia, youtube] → total: 237\n",
      "  ├── [calculator, a calculator, a calculator., counter, calculator (or use excel), computer algebra system, calculator (or ability to count), calculator or counting function] → total: 45\n",
      "  ├── [image recognition tools, image recognition, ocr, video recognition tools, color recognition, image recognition/ocr, audio capability, image recognition tools (to identify and parse a figure with three axes), video parsing, gif parsing tools, video processing software, audio processing software, computer vision, image recognition and processing tools, computer vision or ocr, image search tools, video capability, image processing tools, age recognition software] → total: 44\n",
      "  ├── [pdf access, pdf viewer, excel file access, a file interface, file handling, word document access, xml file access, pdf reader, xlsx file access, powerpoint viewer, csv file access, access to excel files, pdf reader/extracter] → total: 30\n",
      "  ├── [microsoft excel, microsoft excel / google sheets, excel, code/data analysis tools, text editor, spreadsheet editor, tools required, graph interaction tools] → total: 16\n",
      "  ├── [ne] → total: 9\n",
      "  ├── [a speech-to-text tool, tool to extract text from images, a word reversal tool / script, text processing/diff tool, natural language processor, a speech-to-text audio processing tool] → total: 7\n",
      "  ├── [python, python compiler, a python ide, c++ compiler] → total: 6\n",
      "  ├── [google translate access, google maps, access to google maps] → total: 3\n",
      "  ├── [unlambda compiler (optional)] → total: 1\n",
      "  ├── [jsonld file access] → total: 1\n",
      "  ├── [markdown] → total: 1\n",
      "  ├── [bass note data] → total: 1\n",
      "  ├── [bablyonian cuniform -> arabic legend] → total: 1\n",
      "  ├── [rubik's cube model] → total: 1\n"
     ]
    }
   ],
   "source": [
    "grouped_tools = group_similar_tools(tools_counter, threshold=0.7)\n",
    "for label, count in grouped_tools.items():\n",
    "    print(f\"  ├── [{label}] → total: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44706f3c-aed3-4171-b471-ad15c668657a",
   "metadata": {},
   "source": [
    "## Selecting Canonical Labels for Tool Clusters\n",
    "\n",
    "After grouping redundant tools into semantically similar clusters, we now need to select a canonical label for each cluster. A canonical label serves as the representative name for each tool group; for example, `\"search engine\"` might represent all variations of internet search tools like `\"web browser\"`, `\"google search\"`, or `\"access to wikipedia\"`.\n",
    "\n",
    "To do this, we again use a sentence transformer to compute embeddings for each tool name within a cluster. Then, we calculate the centroid vector (average embedding) for the cluster and choose the tool name whose embedding is closest to the centroid in terms of cosine similarity. This ensures that the canonical label is the most semantically central or neutral name among the cluster’s tools.\n",
    "\n",
    "The result is:\n",
    "\n",
    "- canonical_mapping: a dictionary mapping each original tool name to its canonical representative.\n",
    "- canonical_counts: a count of how many times each canonical tool appears, aggregating across all merged names.\n",
    "\n",
    "This mapping simplifies the toolset and creates a clean, standardized vocabulary for downstream tool implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "90e190bd-dc4b-4b49-b446-a6bf92b4f408",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "def get_nlp_canonical_mapping(grouped_clusters, original_counts):\n",
    "    \"\"\"\n",
    "    Uses sentence embeddings to find the most semantically central (canonical) label in each cluster.\n",
    "    \n",
    "    Returns:\n",
    "        canonical_mapping: dict {original_tool → canonical_label}\n",
    "        canonical_counts: dict {canonical_label → total_count}\n",
    "    \"\"\"\n",
    "    model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    \n",
    "    canonical_mapping = {}\n",
    "    canonical_counts = {}\n",
    "\n",
    "    for cluster_label, count in grouped_clusters.items():\n",
    "        tools = [t.strip() for t in cluster_label.split(',')]\n",
    "        embeddings = model.encode(tools)\n",
    "\n",
    "        # Compute centroid of embeddings\n",
    "        centroid = np.mean(embeddings, axis=0)\n",
    "\n",
    "        # Compute cosine similarities to centroid\n",
    "        sims = util.cos_sim(centroid, embeddings)[0]\n",
    "\n",
    "        # Select tool closest to centroid\n",
    "        canonical_idx = sims.argmax().item()\n",
    "        canonical_tool = tools[canonical_idx]\n",
    "\n",
    "        # Map all tools in cluster to canonical\n",
    "        for tool in tools:\n",
    "            canonical_mapping[tool] = canonical_tool\n",
    "        canonical_counts[canonical_tool] = count\n",
    "\n",
    "    return canonical_mapping, canonical_counts\n",
    "\n",
    "canonical_mapping, canonical_counts = get_nlp_canonical_mapping(grouped_tools, tools_counter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "54457f71-9fba-4883-b82f-f4c97a264753",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a calculator',\n",
       " 'a speech-to-text tool',\n",
       " 'access to google maps',\n",
       " 'bablyonian cuniform -> arabic legend',\n",
       " 'bass note data',\n",
       " 'image recognition tools',\n",
       " 'jsonld file access',\n",
       " 'markdown',\n",
       " 'ne',\n",
       " 'pdf access',\n",
       " 'python compiler',\n",
       " \"rubik's cube model\",\n",
       " 'search engine',\n",
       " 'spreadsheet editor',\n",
       " 'unlambda compiler (optional)'}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(canonical_mapping.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66da15e3-9786-4c55-ad16-4e5611a89ca4",
   "metadata": {},
   "source": [
    "## Implementing Canonical Tools for the Agent\n",
    "\n",
    "Now that we have identified a canonical set of tools based on the GAIA dataset, the next step is to implement each tool programmatically so the agent can invoke them when needed. These tools will be wrapped as LangChain-compatible `@tool` functions, allowing LangGraph to automatically call the appropriate function when the agent determines that a tool is required to complete a task.\n",
    "\n",
    "Each tool is designed to match one of the canonical labels we derived earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f27767-c34f-4639-a548-19250efcde51",
   "metadata": {},
   "source": [
    "### Calculator Tool\n",
    "\n",
    "The calculator tool is a fundamental utility used to evaluate arithmetic expressions. In the GAIA dataset, annotators frequently used a calculator to perform basic numerical operations such as addition, multiplication, and division.\n",
    "\n",
    "We implement the calculator as a secure wrapper around Python’s `eval()` function, disabling built-in functions to reduce the risk of arbitrary code execution. The tool takes a string input like `\"10 / (2 + 3)\"` and returns the computed result as a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "04c99739-a200-4329-954c-83096ed9f6ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def calculator(expression: str) -> str:\n",
    "    \"\"\"Evaluates a math expression like '2 + 2 * 3'.\"\"\"\n",
    "    try:\n",
    "        return str(eval(expression, {\"__builtins__\": {}}))\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "# Test\n",
    "print(calculator.invoke(\"10 / (2 + 3)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6de2ea2-22e5-4b9e-a4f0-9d1f12bffbf7",
   "metadata": {},
   "source": [
    "### Web Browser Tool\n",
    "\n",
    "While many GAIA tasks require factual recall, a significant portion also rely on up-to-date information or content that resides online. To simulate this capability within the agent, we implement a web browser tool.\n",
    "\n",
    "This tool mimics how a human might look something up on the internet: by issuing a query to a search engine and scanning the contents of the results. We use the `TavilySearchResults` wrapper from the LangChain community tools, which provides a simple API to retrieve relevant web snippets from trusted sources.\n",
    "\n",
    "This tool is especially useful for fact-checking, resolving ambiguous context, or accessing specific examples or definitions that are not directly encoded in the model’s training data.\n",
    "\n",
    "The tool returns a list of search results, each with a title, URL, content preview, and relevance score. This functionality enables the agent to incorporate recent or external knowledge into its responses, enhancing its ability to reason about real-world queries just as a human would."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a9d01538-359b-4f42-8539-5f5c27b7d2eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'title': 'Who are the consensus top 5 greatest footballers of all time and why?', 'url': 'https://www.reddit.com/r/SoccerNoobs/comments/1ia0b7f/who_are_the_consensus_top_5_greatest_footballers/', 'content': 'Lionel Messi, Cristiano Ronaldo, Pele and Diego Maradona are generally viewed as the top four (not necessarily in that order).', 'score': 0.8215175}, {'title': \"The IFFHS proclaims Messi as 'the best of all time'... and there is ...\", 'url': 'https://www.marca.com/en/football/2025/05/19/682af86122601dea4f8b4573.html', 'content': \"m**essi is the number one in history, at least for the International Federation of Football History and Statistics (IFFHS),** which last weekend drew up its ranking of the 10 best players of all time. And of course, the controversy has erupted in all forums in a discussion that is very footballing and lawful [...] The International Federation of Football History and Statistics published its own ranking\\n\\n![Image 3: The IFFHS proclaims Messi as 'the best of all time'... and there is already controversy](https://e00-marca.uecdn.es/3b6f63a4aa31f25ef86e60580aa927ea/resize/1320/f/jpg/assets/multimedia/imagenes/2025/05/19/17476467965365.jpg)\\n\\n*   JUAN CASTRO \\n*   RYAN MITCHELL \\n\\n 19/05/2025 - 04:29 CDT [...] The ranking **was based on individual and collective statistics of all the players in question.** Messi (Barcelona, PSG, Inter Miami and Argentina) has 46 titles in total, apart from individual ones **(eight Ballon d'Ors, among others). He is the most decorated footballer in the history of the game**: 35 with Barcelona; three with PSG; two with Inter Miami, and six with Argentina (all categories).\", 'score': 0.8170061}, {'title': 'The IFFHS has named Lionel Messi as the greatest player of all time ...', 'url': 'https://www.facebook.com/OneFootball/posts/the-iffhs-has-named-lionel-messi-as-the-greatest-player-of-all-time-%EF%B8%8F/1260956262061377/', 'content': '[](https://www.facebook.com/people/Yusril/pfbid02sy7maX5KoLnWUYhBdVXudGi9VeJ2TJSrWEKW1WpE8VnoRABzFeqzZakS7rTfTtpjl/?comment_id=Y29tbWVudDoxMjYwOTU2MjYyMDYxMzc3XzY5Nzc5OTkxOTMyMDcyNw%3D%3D&__cft__[0]=AZV7aJTUmABra5uYid3cJYkF6DB3w1_G8NyDFrLpu7PKvJTg8-GNCnAnXjeoUF5-U6p5aXiFWk6U_r_U5iL0WGbhy0Z907GgLyX2BsnPI81SKCgkrwkRiGD0iaMtTJz1pKaWu2A-gXpeXvaiuR3tQenoevOX-FZ3qhw5kEHMT0lqDA&__tn__=R]-R) [...] [](https://www.facebook.com/people/Arap-Koros/61557553650661/?comment_id=Y29tbWVudDoxMjYwOTU2MjYyMDYxMzc3XzEwMzMxODE3Nzg3OTA1OTc%3D&__cft__[0]=AZV7aJTUmABra5uYid3cJYkF6DB3w1_G8NyDFrLpu7PKvJTg8-GNCnAnXjeoUF5-U6p5aXiFWk6U_r_U5iL0WGbhy0Z907GgLyX2BsnPI81SKCgkrwkRiGD0iaMtTJz1pKaWu2A-gXpeXvaiuR3tQenoevOX-FZ3qhw5kEHMT0lqDA&__tn__=R]-R)\\n\\n![Image 10](https://static.xx.fbcdn.net/rsrc.php/v4/y9/r/Z-dbClQDXLv.png)OneFootballers [...] View all 57 replies\\n\\n[](https://www.facebook.com/tinax.sbanda.5?comment_id=Y29tbWVudDoxMjYwOTU2MjYyMDYxMzc3XzkzOTU1MDY2MTcyMjUyMTU%3D&__cft__[0]=AZV7aJTUmABra5uYid3cJYkF6DB3w1_G8NyDFrLpu7PKvJTg8-GNCnAnXjeoUF5-U6p5aXiFWk6U_r_U5iL0WGbhy0Z907GgLyX2BsnPI81SKCgkrwkRiGD0iaMtTJz1pKaWu2A-gXpeXvaiuR3tQenoevOX-FZ3qhw5kEHMT0lqDA&__tn__=R]-R)\\n\\n![Image 32](https://static.xx.fbcdn.net/rsrc.php/v4/y9/r/Z-dbClQDXLv.png)OneFootballers', 'score': 0.8127637}, {'title': \"The Top 100: NFL's Greatest Players - Wikipedia\", 'url': 'https://en.wikipedia.org/wiki/The_Top_100:_NFL%27s_Greatest_Players', 'content': 'The final episode, premiering on November 4, 2010, introduced the top 10 players of all time according to the panel.[[2]](#cite_note-2) [Jerry Rice](/wiki/Jerry_Rice \"Jerry Rice\") was chosen as the top player of all time, with [Jim Brown](/wiki/Jim_Brown \"Jim Brown\") as the second choice.[[3]](#cite_note-3)\\n\\n## The list', 'score': 0.7923522}, {'title': 'Top 15 Best NFL Players of All Time - Superprof', 'url': 'https://www.superprof.com/blog/best-nfl-players-all-time/', 'content': 'His most notable career statistics include 1,088 tackles, 132.5 sacks, two Super Bowl wins, an NFL MVP award, and three NFL Defensive Player of the Year awards. He earned many titles in his career, including NFL MVP, NFL Offensive Player of the Year, NFL Man of the Year, and a Super Bowl championship. With 40,551 passing yards, 273 passing touchdowns, and four Super Bowl titles accumulated during his career, “Cool Joe” stands firmly as one of the best football players in NFL history. He racked up many achievements in his pro career including NFL Offensive Rookie of the Year, two times NFL Offensive Player of the Year, NFL MVP, and 10 Pro-Bowl appearances.', 'score': 0.78393924}]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "# Basic browsing simulation (search + preview)\n",
    "@tool\n",
    "def web_browser(query: str) -> str:\n",
    "    \"\"\"Simulates a web browser by performing a search and returning relevant page content.\"\"\"\n",
    "    tavily = TavilySearchResults()\n",
    "    return tavily.run(query)\n",
    "\n",
    "result = web_browser.invoke(\"best football player of all time\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8d2b42-490b-4d8e-9e12-04a6b4920218",
   "metadata": {},
   "source": [
    "### Python Code Execution Tool\n",
    "\n",
    "In some GAIA tasks, reasoning involves more than just retrieving facts, it requires applying logic or performing step-by-step calculations. While the calculator tool helps with simple expressions, it lacks the flexibility for more complex operations like loops, conditionals, or custom logic.\n",
    "\n",
    "To address this, we introduce a Python code execution tool, which allows the agent to safely execute small Python snippets inside a sandboxed environment. This is particularly useful for:\n",
    "\n",
    "- Aggregating values (e.g., sums, averages)\n",
    "- Simulating algorithmic steps\n",
    "- Validating logic or patterns\n",
    "- Parsing and transforming data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fc23f10e-7e28-4101-b3f5-c9c986b11816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n"
     ]
    }
   ],
   "source": [
    "@tool\n",
    "def execute_python(code: str) -> str:\n",
    "    \"\"\"Compiles and executes a Python snippet (sandboxed).\"\"\"\n",
    "    try:\n",
    "        local_vars = {}\n",
    "        exec(code, {}, local_vars)\n",
    "        return str(local_vars.get(\"result\", \"[Executed]\"))\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "# Test\n",
    "print(execute_python.invoke({\"code\": \"result = sum([10, 20, 30])\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04f669a-6a31-4972-b128-969b6ed3b245",
   "metadata": {},
   "source": [
    "### PDF Reading Tool\n",
    "\n",
    "Some GAIA tasks involve documents provided as PDFs including scientific papers, scanned forms, manuals, or legal excerpts. To allow our agent to handle these cases, we introduce a PDF reading tool.\n",
    "\n",
    "This tool uses the `PyMuPDF` library (`fitz`) to open and extract text from the first few pages of a given PDF file. The focus on only the first three pages is intentional: it reduces processing time and ensures fast, focused retrieval, especially for large files.\n",
    "\n",
    "This tool is essential when the agent needs to answer questions based on content embedded in PDFs. For example, a question might ask for a specific clause in a policy document or a quote from a report. Having a tool that can ingest and parse these files makes such tasks feasible.\n",
    "\n",
    "By constraining the text output to the first 1000 characters, we keep the system responsive while still surfacing meaningful context for many practical tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6e056ab9-f0dc-4cd2-b932-449aef87e9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyMuPDF\n",
      "  Downloading pymupdf-1.26.0-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
      "Downloading pymupdf-1.26.0-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (24.1 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: PyMuPDF\n",
      "Successfully installed PyMuPDF-1.26.0\n"
     ]
    }
   ],
   "source": [
    "!pip install PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "049d924d-edcc-4f54-8a50-3f2c7510ff27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "@tool\n",
    "def read_pdf(filename: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts and returns text from the first few pages of a PDF file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with fitz.open(filename) as doc:\n",
    "            text = \"\"\n",
    "            for page in doc[:3]:  # Limit to first 3 pages for speed\n",
    "                text += page.get_text()\n",
    "        return text[:1000]  # Limit output to 1000 characters\n",
    "    except Exception as e:\n",
    "        return f\"Error reading PDF: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febbab2a-560a-4f9e-9f21-2a3a4a5c3e90",
   "metadata": {},
   "source": [
    "### Spreadsheet Reading Tool\n",
    "\n",
    "To allow the agent to extract and reason over spreadsheet resources, I provide a spreadsheet reader capable of handling multiple common formats: CSV, TSV, XLSX, and XLS.\n",
    "The tool below uses pandas to open the file, infer the correct format from its extension, and return the first few rows as a plain-text table. This summary allows the language model to inspect tabular data and extract useful patterns or insights for reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f779ff2a-6060-4949-ab18-2b039756dc01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /home/yurimarca/.pyenv/versions/3.10.16/envs/hf-agents/lib/python3.10/site-packages (2.2.3)\n",
      "Collecting openpyxl\n",
      "  Downloading openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting xlrd\n",
      "  Downloading xlrd-2.0.1-py2.py3-none-any.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /home/yurimarca/.pyenv/versions/3.10.16/envs/hf-agents/lib/python3.10/site-packages (from pandas) (2.2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/yurimarca/.pyenv/versions/3.10.16/envs/hf-agents/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/yurimarca/.pyenv/versions/3.10.16/envs/hf-agents/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/yurimarca/.pyenv/versions/3.10.16/envs/hf-agents/lib/python3.10/site-packages (from pandas) (2025.2)\n",
      "Collecting et-xmlfile (from openpyxl)\n",
      "  Downloading et_xmlfile-2.0.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: six>=1.5 in /home/yurimarca/.pyenv/versions/3.10.16/envs/hf-agents/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
      "Downloading xlrd-2.0.1-py2.py3-none-any.whl (96 kB)\n",
      "Downloading et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: xlrd, et-xmlfile, openpyxl\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [openpyxl]━━━━━━━━━\u001b[0m \u001b[32m2/3\u001b[0m [openpyxl]\n",
      "\u001b[1A\u001b[2KSuccessfully installed et-xmlfile-2.0.0 openpyxl-3.1.5 xlrd-2.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas openpyxl xlrd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e0425d93-36ee-422d-854e-dc80b53c1820",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "@tool\n",
    "def read_spreadsheet(file_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Reads spreadsheet data (CSV, XLSX, XLS, TSV) and returns the first few rows as a formatted string.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        ext = os.path.splitext(file_name)[-1].lower()\n",
    "        \n",
    "        if ext == \".csv\":\n",
    "            df = pd.read_csv(file_name)\n",
    "        elif ext == \".tsv\":\n",
    "            df = pd.read_csv(file_name, sep=\"\\t\")\n",
    "        elif ext in [\".xlsx\", \".xls\"]:\n",
    "            df = pd.read_excel(file_name, engine=\"openpyxl\" if ext == \".xlsx\" else \"xlrd\")\n",
    "        else:\n",
    "            return f\"Unsupported file extension: {ext}\"\n",
    "\n",
    "        return df.head().to_string(index=False)\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error reading spreadsheet: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f4bfb2e2-258a-4a70-8b99-4c7ecb7a4824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m.\u001b[0m\n",
      "├── Interact-with-Evaluation-API.ipynb\n",
      "├── README.md\n",
      "└── \u001b[01;34mresources\u001b[0m\n",
      "    ├── \u001b[00;36m1f975693-876d-457b-a649-393859e79bf3.mp3\u001b[0m\n",
      "    ├── 7bd855d8-463d-4ed5-93ca-5fe35145f733.xlsx\n",
      "    ├── \u001b[00;36m99c9cc74-fdc8-46c6-8f8d-3ce2d3bfeea3.mp3\u001b[0m\n",
      "    ├── \u001b[01;35mcca530fc-4052-43b2-b130-b30968d8aa44.png\u001b[0m\n",
      "    └── f918266a-b3e0-4914-865d-4faa564f1aef.py\n",
      "\n",
      "2 directories, 7 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e5e3d690-0f60-4078-ba0c-40462ab99231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Location  Burgers  Hot Dogs  Salads  Fries  Ice Cream  Soda\n",
      "Pinebrook     1594      1999    2002   2005       1977  1980\n",
      " Wharvton     1983      2008    2014   2015       2017  2018\n",
      "  Sagrada     2019      2022    2022   2023       2021  2019\n",
      "Algrimand     1958      1971    1982   1989       1998  2009\n",
      "  Marztep     2015      2016    2018   2019       2021  2022\n"
     ]
    }
   ],
   "source": [
    "print(read_spreadsheet.invoke({\"file_name\": \"resources/7bd855d8-463d-4ed5-93ca-5fe35145f733.xlsx\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f82530-e95e-4208-97c7-228bdeb9a1f0",
   "metadata": {},
   "source": [
    "### Image Recognition Tool\n",
    "\n",
    "Some queries include images like diagrams, signs, product photos, or interface screenshots, that require visual understanding. To enable the agent to process these visual elements, we integrate a vision transformer-based image recognition tool.\n",
    "This tool uses the pretrained `google/vit-base-patch16-224` model from Hugging Face, which applies transformer-based architectures to classify image content. The model identifies high-level visual features and returns the top three predicted categories for a given image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0b59082b-8602-45c0-994c-22c88db679c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a2d80afe8b9469c9753ffd7c8b3f8ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b5cab93093241459d698ce8629ad435",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/69.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e050b7972f4149bdb8252b6d04026f73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_core.tools import tool\n",
    "from transformers import AutoProcessor, AutoModelForImageClassification\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# Load model and processor once\n",
    "processor = AutoProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "model = AutoModelForImageClassification.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "\n",
    "@tool\n",
    "def recognize_image(image_path: str) -> str:\n",
    "    \"\"\"\n",
    "    Uses a vision transformer to recognize objects in an image file.\n",
    "    Returns top predicted labels.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        inputs = processor(images=image, return_tensors=\"pt\")\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            predicted = torch.topk(logits, k=3).indices.squeeze().tolist()\n",
    "\n",
    "        labels = [model.config.id2label[i] for i in predicted]\n",
    "        return f\"Top predictions: {', '.join(labels)}\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error processing image: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9deedb6d-81ab-44b8-aee6-e865fa67c9b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top predictions: crossword puzzle, crossword, jigsaw puzzle, doormat, welcome mat\n"
     ]
    }
   ],
   "source": [
    "print(recognize_image.invoke({\"image_path\": \"resources/cca530fc-4052-43b2-b130-b30968d8aa44.png\"}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4defbb1f-f309-413d-9661-174ff9d970f6",
   "metadata": {},
   "source": [
    "# Constructing the LangGraph Agent\n",
    "\n",
    "Here, I combine all previous components into a **LangGraph-based autonomous agent**. The agent is capable of retrieving relevant documents, invoking external tools, and reasoning over complex queries using a structured state machine.\n",
    "\n",
    "## Step 1: Import Dependencies and Load Environment Variables\n",
    "\n",
    "We begin by importing essential modules:\n",
    "\n",
    "* `LangGraph`, `LangChain`, and `LangChain Core` components to build the agent graph and manage messages.\n",
    "* `FAISS` for local vector-based retrieval.\n",
    "* `HuggingFaceEmbeddings` to embed text documents into vector space.\n",
    "\n",
    "## Step 2: Define the System Prompt\n",
    "\n",
    "A detailed **system prompt** is prepared to instruct the LLM on how to behave. It defines:\n",
    "\n",
    "* The agent’s role and tools.\n",
    "* Best practices for answering (e.g., reasoning step by step).\n",
    "* When to use tools vs. answer directly.\n",
    "* Behavior in case of ambiguity.\n",
    "\n",
    "This prompt ensures consistency and improves interpretability of the agent’s behavior.\n",
    "\n",
    "## Step 3: Initialize the Retriever with FAISS\n",
    "\n",
    "To support long-term memory and context retrieval, we:\n",
    "\n",
    "* Embed example documents using the `all-mpnet-base-v2` model.\n",
    "* Store these embeddings in **FAISS**, an efficient in-memory vector database.\n",
    "* Define a `retriever` function that searches for relevant docs based on the latest user message and appends them to the conversation context.\n",
    "\n",
    "This retrieval mechanism helps the agent ground its responses in known facts.\n",
    "\n",
    "## Step 4: Register Available Tools\n",
    "\n",
    "The agent is equipped with the following tools:\n",
    "\n",
    "*  `calculator`: Evaluate math expressions.\n",
    "*  `read_pdf`: Extract text from PDF files.\n",
    "*  `read_spreadsheet`: Read and summarize spreadsheet data.\n",
    "*  `recognize_image`: Label images using a Vision Transformer.\n",
    "*  `execute_python`: Run Python code snippets in a sandbox.\n",
    "*  `web_browser`: Perform live web searches using Tavily.\n",
    "\n",
    "\n",
    "## Step 5: Define the Assistant Node\n",
    "\n",
    "The `assistant` node calls the language model (ChatOllama with Qwen2.5) to generate a response based on:\n",
    "\n",
    "* The current conversation state.\n",
    "* Any documents retrieved earlier.\n",
    "* The system prompt and tool results.\n",
    "\n",
    "## Step 6: Build the LangGraph Agent\n",
    "\n",
    "Using `StateGraph`, we construct a flowchart-like agent:\n",
    "\n",
    "```python\n",
    "builder = StateGraph(AgentState)\n",
    "```\n",
    "\n",
    "The graph includes three nodes:\n",
    "\n",
    "1. `\"retriever\"` → adds context from FAISS.\n",
    "2. `\"assistant\"` → generates a response or decides to use a tool.\n",
    "3. `\"tools\"` → executes the chosen tool.\n",
    "\n",
    "**Graph edges** define execution order:\n",
    "\n",
    "* Start → Retriever\n",
    "* Retriever → Assistant\n",
    "* Assistant → Tool (conditionally)\n",
    "* Tool → Assistant (feedback loop)\n",
    "\n",
    "This creates a flexible feedback cycle that allows the agent to use multiple tools if necessary before responding.\n",
    "\n",
    "## Final Step: Compile the Agent\n",
    "\n",
    "This step transforms the abstract graph into a runnable agent. The compiled agent can now handle user queries, reason through problems, look up context, and call tools—all in a structured and interpretable manner.\n",
    "\n",
    "We now have a fully functional **tool-augmented LLM agent**, powered by LangGraph, capable of:\n",
    "\n",
    "* **Retrieval-Augmented Generation (RAG)** using FAISS.\n",
    "* **Tool-calling decisions** made dynamically by the LLM.\n",
    "* **Sequential reasoning** controlled via graph transitions.\n",
    "\n",
    "This architecture is modular, interpretable, and easily extensible to more sophisticated reasoning workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b06dc3e2-f582-4cb1-b98e-562b5bb8d068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0ec60bac53d471e806a651107a63afe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0537ff9e32d4468f869a813f0876d15d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70e98bb760ce4833add49058e1006286",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.4k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3db26682af847149a0b0bd61571a836",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "053aeb10fa594ac4bb731dbd2981f4b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c72b2ede0944741af70071fc2f2a969",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea2f83608a0b48caacc0f25ad8aa3bea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b8b43d50bd442acbc48fb6efc0c2ae0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e76d5ed9bb2424b81456119ce65209b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "549e373af1024de088ced753ec7d835d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8b4384d7101499d93ce55ac3fa46f66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langgraph.graph import StateGraph, START\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langchain_core.messages import SystemMessage, HumanMessage, AnyMessage\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.docstore.document import Document\n",
    "from typing import TypedDict, Annotated\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# 1. Prepare system prompt\n",
    "system_prompt = \"\"\"\n",
    "You are an expert AI assistant with the ability to use external tools when needed. Your tools include:\n",
    "\n",
    "- Calculator\n",
    "- PDF reader\n",
    "- Spreadsheet reader\n",
    "- OCR for reading text from images\n",
    "- Python code executor\n",
    "- Web browser for retrieving current information\n",
    "\n",
    "When solving problems:\n",
    "- Think step by step.\n",
    "- Use the appropriate tools whenever the information is not immediately available or requires calculation, document reading, or web searching.\n",
    "- If the answer is obvious or can be answered based on prior knowledge, respond directly.\n",
    "- Always explain your reasoning when helpful.\n",
    "- If a task is ambiguous, ask clarifying questions.\n",
    "\n",
    "Your goal is to provide accurate, helpful, and well-reasoned answers.\n",
    "\"\"\"\n",
    "sys_msg = SystemMessage(content=system_prompt)\n",
    "\n",
    "# 2. Setup embeddings and vector store for retriever (local with FAISS)\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "\n",
    "# Example documents for the vector store\n",
    "docs = [\n",
    "    Document(page_content=\"LangGraph is a framework for building stateful AI agents.\"),\n",
    "    Document(page_content=\"LangChain provides a standard interface for chains, agents, and tools.\"),\n",
    "    Document(page_content=\"FAISS is a library for efficient similarity search.\"),\n",
    "]\n",
    "\n",
    "# Create a FAISS vector store in memory\n",
    "vector_store = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "# 3. Define retriever node\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "\n",
    "def retriever(state: AgentState):\n",
    "    query = state[\"messages\"][-1].content\n",
    "    similar_docs = vector_store.similarity_search(query, k=2)\n",
    "    reference_msg = HumanMessage(\n",
    "        content=(\n",
    "            f\"Reference documents:\\n\" +\n",
    "            \"\\n\\n\".join([doc.page_content for doc in similar_docs]) if similar_docs else \"No relevant documents found.\"\n",
    "        )\n",
    "    )\n",
    "    return {\"messages\": [sys_msg] + state[\"messages\"] + [reference_msg]}\n",
    "\n",
    "# 4. Setup LLM with tools\n",
    "tools = [calculator, read_pdf, read_spreadsheet, recognize_image, execute_python, web_browser]\n",
    "llm = ChatOllama(model=\"qwen2.5:32b\", temperature=0)\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "# 5. Assistant node\n",
    "def assistant(state: AgentState):\n",
    "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
    "\n",
    "# 6. Build graph\n",
    "builder = StateGraph(AgentState)\n",
    "builder.add_node(\"retriever\", retriever)\n",
    "builder.add_node(\"assistant\", assistant)\n",
    "builder.add_node(\"tools\", ToolNode(tools))\n",
    "\n",
    "builder.add_edge(START, \"retriever\")\n",
    "builder.add_edge(\"retriever\", \"assistant\")\n",
    "builder.add_conditional_edges(\"assistant\", tools_condition)\n",
    "builder.add_edge(\"tools\", \"assistant\")\n",
    "\n",
    "# Compile the graph agent\n",
    "agent = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e3e13628-6e47-4b6b-b0cd-a51a46498375",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "compiled_graph = builder.compile()\n",
    "display(Image(compiled_graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574a678b-87e7-4437-8426-beabdfa24e5b",
   "metadata": {},
   "source": [
    "## 24. Testing the Agent on a Real GAIA Sample\n",
    "\n",
    "In this final section, we validate the full pipeline by testing our agent on a randomly selected question from the **GAIA dataset**. This test simulates the full interaction flow—from ingesting a real-world task, to retrieving contextual documents, invoking tools, and generating a reasoned final response.\n",
    "\n",
    "### Step-by-Step Breakdown\n",
    "\n",
    "* **Formatting the Sample**:\n",
    "  We use a `print_sample` helper function to neatly display all metadata from a GAIA sample, including:\n",
    "\n",
    "  * Task ID and question level.\n",
    "  * The natural language **question**.\n",
    "  * The **annotator's final answer**.\n",
    "  * Step-by-step reasoning and tools used during the annotation.\n",
    "\n",
    "* **Selecting the Test Query**:\n",
    "  A single question is randomly chosen from the dataset using:\n",
    "\n",
    "  ```python\n",
    "  sample = random.choice(dataset.to_list())\n",
    "  ```\n",
    "\n",
    "* **Invoking the Agent**:\n",
    "  The selected question is passed to the compiled LangGraph agent, and the total response time is measured with `time.time()`. This simulates a real inference use case:\n",
    "\n",
    "  ```python\n",
    "  response = agent.invoke({\"messages\": [{\"type\": \"user\", \"content\": question}]})\n",
    "  ```\n",
    "\n",
    "* **Displaying the Result**:\n",
    "  The final assistant response is printed along with the runtime duration, so we can assess:\n",
    "\n",
    "  * The **quality** of the generated answer.\n",
    "  * Whether the agent invoked the appropriate tools.\n",
    "  * How closely the output matches the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "063110e1-78f7-4ebe-ac6b-d97b25e3b325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────\n",
      "Task ID     : 20194330-9976-4043-8632-f8485c6c71b2\n",
      "Level       : 2\n",
      "Question    : The YouTube channel Game Grumps began a Let’s Play of the game Sonic the Hedgehog (2006) in the year 2012. Thirty seconds into the first episode, a phrase is shown on the screen in white letters on a red background. How many times does the letter \"E\" appear in this phrase?\n",
      "Answer      : 4\n",
      "Annotator Metadata:\n",
      " ├── Steps:\n",
      " │    ├── 1. Look up \"Game grumps sonic 2006 playthrough\".\n",
      " │    ├── 2. Click on the first result and verify that it matches the parameters from the question.\n",
      " │    ├── 3. Scrub to the thirty-second mark in the video.\n",
      " │    ├── 4. Note the letters in white on the red background.\n",
      " │    ├── 5. Count the letter \"E\"'s in the phrase.\n",
      " ├── Number of steps : 5\n",
      " ├── Duration        : 5 minutes\n",
      " ├── Tools used:\n",
      " │    ├── 1. Web browser\n",
      " │    ├── 2. YouTube player\n",
      " │    ├── 3. Color recognition\n",
      " │    ├── 4. OCR\n",
      " └── Number of tools : 4\n",
      "────────────────────────────────────────────────────────────\n",
      "\n",
      "Agent's Response:\n",
      "\n",
      "Agent's Response (runtime: 58.54 seconds):\n",
      "Based on the search results, there isn't a direct quote or phrase from the first episode of Game Grumps' Sonic the Hedgehog (2006) Let's Play that appears in white letters on a red background within the provided information. To find out how many times the letter \"E\" appears in this specific phrase, we would need to watch the video starting at 30 seconds into the first episode.\n",
      "\n",
      "Would you like me to provide a direct link to the video or attempt to describe the process of finding that exact phrase?\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Function to pretty print the sample\n",
    "def print_sample(sample):\n",
    "    line = \"─\" * 60\n",
    "    print(line)\n",
    "    print(f\"Task ID     : {sample['task_id']}\")\n",
    "    print(f\"Level       : {sample['Level']}\")\n",
    "    print(f\"Question    : {sample['Question']}\")\n",
    "    print(f\"Answer      : {sample['Final answer']}\")\n",
    "    print(\"Annotator Metadata:\")\n",
    "    print(\" ├── Steps:\")\n",
    "    for step in sample['Annotator Metadata']['Steps'].split('\\n'):\n",
    "        print(f\" │    ├── {step}\")\n",
    "    print(f\" ├── Number of steps : {sample['Annotator Metadata']['Number of steps']}\")\n",
    "    print(f\" ├── Duration        : {sample['Annotator Metadata']['How long did this take?']}\")\n",
    "    print(f\" ├── Tools used:\")\n",
    "    for tool in sample['Annotator Metadata']['Tools'].split('\\n'):\n",
    "        print(f\" │    ├── {tool}\")\n",
    "    print(f\" └── Number of tools : {sample['Annotator Metadata']['Number of tools']}\")\n",
    "    print(line)\n",
    "\n",
    "# Sample one question from dataset\n",
    "sample = random.choice(dataset.to_list())\n",
    "\n",
    "# Show sample details\n",
    "print_sample(sample)\n",
    "\n",
    "# Extract the question text\n",
    "question = sample[\"Question\"]\n",
    "\n",
    "# Invoke the agent with the question\n",
    "start = time.time()\n",
    "response = agent.invoke({\n",
    "    \"messages\": [\n",
    "        {\"type\": \"user\", \"content\": question}\n",
    "    ]\n",
    "})\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "\n",
    "# Print the agent's response\n",
    "print(\"\\nAgent's Response:\")\n",
    "print(f\"\\nAgent's Response (runtime: {elapsed:.2f} seconds):\")\n",
    "print(response['messages'][-1].content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "a8de3661-648d-4b51-80fc-2bd02920b064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "────────────────────────────────────────────────────────────\n",
      "Task ID     : dd3c7503-f62a-4bd0-9f67-1b63b94194cc\n",
      "Level       : 2\n",
      "Question    : Use density measures from the chemistry materials licensed by Marisa Alviar-Agnew & Henry Agnew under the CK-12 license in LibreText's Introductory Chemistry materials as compiled 08/21/2023.\n",
      "\n",
      "I have a gallon of honey and a gallon of mayonnaise at 25C. I remove one cup of honey at a time from the gallon of honey. How many times will I need to remove a cup to have the honey weigh less than the mayonaise? Assume the containers themselves weigh the same.\n",
      "Answer      : 6\n",
      "Annotator Metadata:\n",
      " ├── Steps:\n",
      " │    ├── 1. Search \"LibreText density mayonnaise\"\n",
      " │    ├── 2. Click result, confirm the correct license.\n",
      " │    ├── 3. Search \"cm^3 to 1 cup\"\n",
      " │    ├── 4. Use results with density measures to form the equation (16*236.588)(1.420 - 0.910)/(236.588*1.420)\n",
      " │    ├── 5. Round up\n",
      " ├── Number of steps : 5\n",
      " ├── Duration        : 20 minutes\n",
      " ├── Tools used:\n",
      " │    ├── 1. Search engine\n",
      " │    ├── 2. Web browser\n",
      " │    ├── 3. Calculator\n",
      " └── Number of tools : 3\n",
      "────────────────────────────────────────────────────────────\n",
      "\n",
      "Agent's Response:\n",
      "\n",
      "Agent's Response (runtime: 275.23 seconds):\n",
      "To solve this problem, we need to compare the weights of honey and mayonnaise when measured in cups.\n",
      "\n",
      "Firstly, let's establish the densities:\n",
      "- Honey has a density around 1.42 g/mL.\n",
      "- Mayonnaise has a density around 0.95 g/mL.\n",
      "\n",
      "A gallon is approximately 3785.41 mL. Therefore, we can calculate the weight of one gallon (in grams) for both honey and mayonnaise:\n",
      "\n",
      "- Weight of honey in one gallon: \\( 3785.41 \\text{ mL} \\times 1.42 \\frac{\\text{g}}{\\text{mL}} = 5375.2822 \\text{ g} \\)\n",
      "- Weight of mayonnaise in one gallon: \\( 3785.41 \\text{ mL} \\times 0.95 \\frac{\\text{g}}{\\text{mL}} = 3596.1395 \\text{ g} \\)\n",
      "\n",
      "Next, we need to find out the weight of one cup (approximately 236.588 mL) for both honey and mayonnaise:\n",
      "\n",
      "- Weight of honey in one cup: \\( 236.588 \\text{ mL} \\times 1.42 \\frac{\\text{g}}{\\text{mL}} = 335.955 \\text{ g} \\)\n",
      "\n",
      "Now, we need to determine how many times we must remove one cup of honey from the gallon so that its weight is less than the weight of a gallon of mayonnaise.\n",
      "\n",
      "The initial difference in weight between the gallon of honey and the gallon of mayonnaise is:\n",
      "\\[ 5375.2822 \\text{ g} - 3596.1395 \\text{ g} = 1779.1427 \\text{ g} \\]\n",
      "\n",
      "Each time we remove one cup (335.955 grams) of honey, the weight difference decreases by that amount.\n",
      "\n",
      "To find out how many times we need to remove a cup:\n",
      "\\[ \\frac{1779.1427}{335.955} \\approx 5.30 \\]\n",
      "\n",
      "Since we can only remove whole cups, we round up to the next whole number because even one more removal will ensure that the honey weighs less than the mayonnaise.\n",
      "\n",
      "Therefore, you need to remove **6** cups of honey from the gallon so that its weight is less than the weight of a gallon of mayonnaise.\n"
     ]
    }
   ],
   "source": [
    "# Sample one question from dataset\n",
    "sample = random.choice(dataset.to_list())\n",
    "\n",
    "# Show sample details\n",
    "print_sample(sample)\n",
    "\n",
    "# Extract the question text\n",
    "question = sample[\"Question\"]\n",
    "\n",
    "# Invoke the agent with the question\n",
    "start = time.time()\n",
    "response = agent.invoke({\n",
    "    \"messages\": [\n",
    "        {\"type\": \"user\", \"content\": question}\n",
    "    ]\n",
    "})\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "\n",
    "# Print the agent's response\n",
    "print(\"\\nAgent's Response:\")\n",
    "print(f\"\\nAgent's Response (runtime: {elapsed:.2f} seconds):\")\n",
    "print(response['messages'][-1].content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf38a44-ecb3-4ddf-8797-32af7f35f0f2",
   "metadata": {},
   "source": [
    "### What Does This Demonstrate?\n",
    "\n",
    "This test closes the loop by showing the **end-to-end performance** of the agent on an actual annotation task. In the example shown, the agent:\n",
    "\n",
    "* Correctly interprets the problem.\n",
    "* Uses **density values** to calculate the weight of each substance.\n",
    "* Performs mathematical reasoning with unit conversions and rounding.\n",
    "* Produces a **concise and correct final answer** (removing 6 cups of honey).\n",
    "\n",
    "By passing this test case, we verify that the graph structure, toolset, retrieval logic, and LLM behavior work harmoniously to emulate human annotator performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159782fc-8445-4ef7-874b-e4a9f6559fc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
